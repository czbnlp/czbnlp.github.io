---
layout: post
title:  LLM-分词方法 (tokenization)
subtitle: 分词方法（WordPiece，Byte-Pair Encoding (BPE)，Byte-level BPE(BBPE)原理及其代码实现）
data:   2024-11-02
author: czb
header-img: img/post-bg-cook.jpg
catalog: true
tags:
    - LLM
comments: true

---

## 前言

LLM中使用的分词方式是面试中常见的问题，本文将介绍主要的分词方法，主要借鉴于[LLMBook](https://llmbook-zh.github.io/).

## 正文

### 字节对编码（BPE, Byte Pair Encoding）
BPE 算法从一组基本符号(例如字母和边界字符)开始，迭代地寻找语料库中的两个相邻词元， 并将它们替换为新的词元，这一过程被称为合并。合并的选择标准是计算两个连 续词元的共现频率，也就是每次迭代中，最频繁出现的一对词元会被选择与合并。 合并过程将一直持续达到预定义的词表大小。

代表性语言模型包括 GPT-2 、BART 和 LLaMA。例如，GPT-2 的词表大小为 50,257 ，包括 256 个字节的基本词元、一个特殊的文 末词元以及通过 50,000 次合并学习到的词元。
### 算法过程

1. 准备语料库，确定期望的subword词表大小等参数
2. 通常在每个单词末尾添加后缀 `</w>`, 并统计每个单词的词频，例如：`hello` 在语料库中出现了10次，则将其按照初始的词表拆分为`h e l l o </w>`，其中`</w>`表示单词的结束, 其意义在于表名subword是词后缀。 举例来说：`st`不加`</w>`可以出现在词首，如`st ar`; 加了`</w>`则表名该subword位于词尾，如`we st</w>`, 二者意义截然不同
3. 统计词表中所有相邻词元的共现频率，例如：`l l`出现的次数最多，则`l l`会被合并为`ll`，并更新词表。
4. 重复步骤2-3，直到词表中的词数小于等于指定的词表大小。

代码演示

```python
import re
from collections import Counter

def extract_frequencies(sequence): 
    """
    Given a list of strings, calculates the frequency of words and returns a dictionary mapping words to frequencies.
    """
    token_counter = Counter() 
    for item in sequence:
        # Tokenize words and add </w> to each word to mark end-of-word
        tokens = ' '.join(list(item)) + ' </w>'
        token_counter[tokens] += 1 
    return token_counter

def frequency_of_pairs(frequencies):
    """
    Given a frequency dictionary, returns a dictionary mapping character pairs to their frequency in the text.
    """
    pairs_count = Counter()
    for token, count in frequencies.items():
        chars = token.split()
        for i in range(len(chars) - 1):
            pair = (chars[i], chars[i+1])
            pairs_count[pair] += count 
    return pairs_count

def merge_vocab(merge_pair, vocab): 
    """
    Merges the most common adjacent character pair in the vocabulary, updating the tokens in the vocabulary.
    """
    re_pattern = re.escape(' '.join(merge_pair))
    pattern = re.compile(r'(?<!\S)' + re_pattern + r'(?!\S)') 
    updated_tokens = {pattern.sub(''.join(merge_pair), token): freq for token, freq in vocab.items()} 
    return updated_tokens

def encode_with_bpe(texts, iterations):
    """
    Given a list of sentences and the number of merge iterations, applies BPE and returns the vocabulary.
    """
    vocab_map = extract_frequencies(texts)
    # print(vocab_map)
    for i in range(iterations):
        
        pair_freqs = frequency_of_pairs(vocab_map)
        if not pair_freqs:
            break
        most_common_pair = pair_freqs.most_common(1)[0][0]
        print(f"第{i}次: 合并的pair为: {most_common_pair}, 更新后的vocab: {vocab_map}")
        vocab_map = merge_vocab(most_common_pair, vocab_map)
    return vocab_map

# Sample data as a list of words to process
data = ["hello", "world", "loop", "loot", "tool"]
num_merges = 10
bpe_pairs = encode_with_bpe(data, num_merges)

print(bpe_pairs)

```

输出结果

```python
第0次: 合并的pair为: ('l', 'o'), 更新后的vocab: Counter({'h e l l o </w>': 1, 'w o r l d </w>': 1, 'l o o p </w>': 1, 'l o o t </w>': 1, 't o o l </w>': 1})
第1次: 合并的pair为: ('lo', 'o'), 更新后的vocab: {'h e l lo </w>': 1, 'w o r l d </w>': 1, 'lo o p </w>': 1, 'lo o t </w>': 1, 't o o l </w>': 1}
第2次: 合并的pair为: ('h', 'e'), 更新后的vocab: {'h e l lo </w>': 1, 'w o r l d </w>': 1, 'loo p </w>': 1, 'loo t </w>': 1, 't o o l </w>': 1}
第3次: 合并的pair为: ('he', 'l'), 更新后的vocab: {'he l lo </w>': 1, 'w o r l d </w>': 1, 'loo p </w>': 1, 'loo t </w>': 1, 't o o l </w>': 1}
第4次: 合并的pair为: ('hel', 'lo'), 更新后的vocab: {'hel lo </w>': 1, 'w o r l d </w>': 1, 'loo p </w>': 1, 'loo t </w>': 1, 't o o l </w>': 1}
第5次: 合并的pair为: ('hello', '</w>'), 更新后的vocab: {'hello </w>': 1, 'w o r l d </w>': 1, 'loo p </w>': 1, 'loo t </w>': 1, 't o o l </w>': 1}
第6次: 合并的pair为: ('w', 'o'), 更新后的vocab: {'hello</w>': 1, 'w o r l d </w>': 1, 'loo p </w>': 1, 'loo t </w>': 1, 't o o l </w>': 1}
第7次: 合并的pair为: ('wo', 'r'), 更新后的vocab: {'hello</w>': 1, 'wo r l d </w>': 1, 'loo p </w>': 1, 'loo t </w>': 1, 't o o l </w>': 1}
第8次: 合并的pair为: ('wor', 'l'), 更新后的vocab: {'hello</w>': 1, 'wor l d </w>': 1, 'loo p </w>': 1, 'loo t </w>': 1, 't o o l </w>': 1}
第9次: 合并的pair为: ('worl', 'd'), 更新后的vocab: {'hello</w>': 1, 'worl d </w>': 1, 'loo p </w>': 1, 'loo t </w>': 1, 't o o l </w>': 1}
{'hello</w>': 1, 'world </w>': 1, 'loo p </w>': 1, 'loo t </w>': 1, 't o o l </w>': 1}


```